{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requirements\n",
    "functions-framework==3.*\n",
    "google-cloud-dataproc\n",
    "google-cloud-storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main.py\n",
    "from google.cloud import dataproc_v1 as dataproc\n",
    "from google.cloud.dataproc_v1.types import Job, PySparkJob\n",
    "\n",
    "def process_file3(data, context):\n",
    "    # Extract information from the Cloud Storage event\n",
    "    bucket = data['bucket']\n",
    "    file_name = data['name']\n",
    "\n",
    "    # Check if the loaded file is the one you want\n",
    "    if file_name == 'tip.json':\n",
    "        # Configure the Dataproc client with the correct endpoint\n",
    "        client = dataproc.JobControllerClient(client_options={\n",
    "            \"api_endpoint\": \"us-central1-dataproc.googleapis.com:443\"\n",
    "        })\n",
    "\n",
    "        # Configure the PySpark job (adjust as needed)\n",
    "        pyspark_job = PySparkJob(main_python_file_uri='gs://bucket_prometheus_sc/ETL_functions/etl_yelp_tip_prueba.py')\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Configure the entire job\n",
    "        job = Job(\n",
    "            pyspark_job=pyspark_job,\n",
    "            # Other job configuration parameters if needed\n",
    "        )\n",
    "\n",
    "        # Specify the cluster where the job will run\n",
    "        cluster_name = \"cluster-etl-spark\"\n",
    "\n",
    "        # Submit the job to Dataproc and get the operation\n",
    "        operation = client.submit_job(\n",
    "            request={\n",
    "                \"project_id\": \"prometheus-data-solutions\",\n",
    "                \"region\": \"us-central1\",\n",
    "                \"job\": {\n",
    "                    \"placement\": {\"cluster_name\": cluster_name},\n",
    "                    \"pyspark_job\": pyspark_job,\n",
    "                    # Other job configuration parameters if needed\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Wait for the operation to complete (blocking call)\n",
    "        while not operation.done:\n",
    "            pass  # You can add sleep here to avoid busy waiting\n",
    "\n",
    "        # Retrieve the result of the operation\n",
    "        result = operation.result()\n",
    "\n",
    "        print(f\"Job completed for the file {file_name}\")\n",
    "    else:\n",
    "        print(f\"No job configured for the file {file_name}\")\n",
    "\n",
    "    if file_name == 'review.json':\n",
    "        # Configure the Dataproc client with the correct endpoint\n",
    "        client = dataproc.JobControllerClient(client_options={\n",
    "            \"api_endpoint\": \"us-central1-dataproc.googleapis.com:443\"\n",
    "        })\n",
    "\n",
    "        # Configure the PySpark job (adjust as needed)\n",
    "        pyspark_job = PySparkJob(main_python_file_uri='gs://bucket_prometheus_sc/ETL_functions/etl_yelp_review.py')    \n",
    "\n",
    "        # Configure the entire job\n",
    "        job = Job(\n",
    "            pyspark_job=pyspark_job,\n",
    "            # Other job configuration parameters if needed\n",
    "        )\n",
    "\n",
    "        # Specify the cluster where the job will run\n",
    "        cluster_name = \"cluster-etl-spark\"\n",
    "\n",
    "        # Submit the job to Dataproc and get the operation\n",
    "        operation = client.submit_job(\n",
    "            request={\n",
    "                \"project_id\": \"prometheus-data-solutions\",\n",
    "                \"region\": \"us-central1\",\n",
    "                \"job\": {\n",
    "                    \"placement\": {\"cluster_name\": cluster_name},\n",
    "                    \"pyspark_job\": pyspark_job,\n",
    "                    # Other job configuration parameters if needed\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Wait for the operation to complete (blocking call)\n",
    "        while not operation.done:\n",
    "            pass  # You can add sleep here to avoid busy waiting\n",
    "\n",
    "        # Retrieve the result of the operation\n",
    "        result = operation.result()\n",
    "\n",
    "        print(f\"Job completed for the file {file_name}\")\n",
    "    else:\n",
    "        print(f\"No job configured for the file {file_name}\")\n",
    "\n",
    "    if file_name == 'user.parquet':\n",
    "        # Configure the Dataproc client with the correct endpoint\n",
    "        client = dataproc.JobControllerClient(client_options={\n",
    "            \"api_endpoint\": \"us-central1-dataproc.googleapis.com:443\"\n",
    "        })\n",
    "\n",
    "        # Configure the PySpark job (adjust as needed)\n",
    "        pyspark_job = PySparkJob(main_python_file_uri='gs://bucket_prometheus_sc/ETL_functions/etl_yelp_user.py') \n",
    " \n",
    "\n",
    "        # Configure the entire job\n",
    "        job = Job(\n",
    "            pyspark_job=pyspark_job,\n",
    "            # Other job configuration parameters if needed\n",
    "        )\n",
    "\n",
    "        # Specify the cluster where the job will run\n",
    "        cluster_name = \"cluster-etl-spark\"\n",
    "\n",
    "        # Submit the job to Dataproc and get the operation\n",
    "        operation = client.submit_job(\n",
    "            request={\n",
    "                \"project_id\": \"prometheus-data-solutions\",\n",
    "                \"region\": \"us-central1\",\n",
    "                \"job\": {\n",
    "                    \"placement\": {\"cluster_name\": cluster_name},\n",
    "                    \"pyspark_job\": pyspark_job,\n",
    "                    # Other job configuration parameters if needed\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Wait for the operation to complete (blocking call)\n",
    "        while not operation.done:\n",
    "            pass  # You can add sleep here to avoid busy waiting\n",
    "\n",
    "        # Retrieve the result of the operation\n",
    "        result = operation.result()\n",
    "\n",
    "        print(f\"Job completed for the file {file_name}\")\n",
    "    else:\n",
    "        print(f\"No job configured for the file {file_name}\")\n",
    "\n",
    "    if file_name == 'checkin.json':\n",
    "        # Configure the Dataproc client with the correct endpoint\n",
    "        client = dataproc.JobControllerClient(client_options={\n",
    "            \"api_endpoint\": \"us-central1-dataproc.googleapis.com:443\"\n",
    "        })\n",
    "\n",
    "        # Configure the PySpark job (adjust as needed)\n",
    "        pyspark_job = PySparkJob(main_python_file_uri='gs://bucket_prometheus_sc/ETL_functions/etl_yelp_checkin.py')\n",
    "\n",
    "        # Configure the entire job\n",
    "        job = Job(\n",
    "            pyspark_job=pyspark_job,\n",
    "            # Other job configuration parameters if needed\n",
    "        )\n",
    "\n",
    "        # Specify the cluster where the job will run\n",
    "        cluster_name = \"cluster-etl-spark\"\n",
    "\n",
    "        # Submit the job to Dataproc and get the operation\n",
    "        operation = client.submit_job(\n",
    "            request={\n",
    "                \"project_id\": \"prometheus-data-solutions\",\n",
    "                \"region\": \"us-central1\",\n",
    "                \"job\": {\n",
    "                    \"placement\": {\"cluster_name\": cluster_name},\n",
    "                    \"pyspark_job\": pyspark_job,\n",
    "                    # Other job configuration parameters if needed\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Wait for the operation to complete (blocking call)\n",
    "        while not operation.done:\n",
    "            pass  # You can add sleep here to avoid busy waiting\n",
    "\n",
    "        # Retrieve the result of the operation\n",
    "        result = operation.result()\n",
    "\n",
    "        print(f\"Job completed for the file {file_name}\")\n",
    "    else:\n",
    "        print(f\"No job configured for the file {file_name}\")\n",
    "\n",
    "    if file_name == 'business.pkl':\n",
    "        # Configure the Dataproc client with the correct endpoint\n",
    "        client = dataproc.JobControllerClient(client_options={\n",
    "            \"api_endpoint\": \"us-central1-dataproc.googleapis.com:443\"\n",
    "        })\n",
    "\n",
    "        # Configure the PySpark job (adjust as needed)\n",
    "        pyspark_job = PySparkJob(main_python_file_uri='gs://bucket_prometheus_sc/ETL_functions/etl_yelp_business.py') \n",
    "\n",
    "        # Configure the entire job\n",
    "        job = Job(\n",
    "            pyspark_job=pyspark_job,\n",
    "            # Other job configuration parameters if needed\n",
    "        )\n",
    "\n",
    "        # Specify the cluster where the job will run\n",
    "        cluster_name = \"cluster-etl-spark\"\n",
    "\n",
    "        # Submit the job to Dataproc and get the operation\n",
    "        operation = client.submit_job(\n",
    "            request={\n",
    "                \"project_id\": \"prometheus-data-solutions\",\n",
    "                \"region\": \"us-central1\",\n",
    "                \"job\": {\n",
    "                    \"placement\": {\"cluster_name\": cluster_name},\n",
    "                    \"pyspark_job\": pyspark_job,\n",
    "                    # Other job configuration parameters if needed\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Wait for the operation to complete (blocking call)\n",
    "        while not operation.done:\n",
    "            pass  # You can add sleep here to avoid busy waiting\n",
    "\n",
    "        # Retrieve the result of the operation\n",
    "        result = operation.result()\n",
    "\n",
    "        print(f\"Job completed for the file {file_name}\")\n",
    "    else:\n",
    "        print(f\"No job configured for the file {file_name}\")\n",
    "\n",
    "    if file_name == 'm9.json':\n",
    "        # Configure the Dataproc client with the correct endpoint\n",
    "        client = dataproc.JobControllerClient(client_options={\n",
    "            \"api_endpoint\": \"us-central1-dataproc.googleapis.com:443\"\n",
    "        })\n",
    "\n",
    "        # Configure the PySpark job (adjust as needed)\n",
    "        pyspark_job = PySparkJob(main_python_file_uri='gs://bucket_prometheus_sc/ETL_functions/etl_google_metadata.py') \n",
    "\n",
    "        # Configure the entire job\n",
    "        job = Job(\n",
    "            pyspark_job=pyspark_job,\n",
    "            # Other job configuration parameters if needed\n",
    "        )\n",
    "\n",
    "        # Specify the cluster where the job will run\n",
    "        cluster_name = \"cluster-etl-spark\"\n",
    "\n",
    "        # Submit the job to Dataproc and get the operation\n",
    "        operation = client.submit_job(\n",
    "            request={\n",
    "                \"project_id\": \"prometheus-data-solutions\",\n",
    "                \"region\": \"us-central1\",\n",
    "                \"job\": {\n",
    "                    \"placement\": {\"cluster_name\": cluster_name},\n",
    "                    \"pyspark_job\": pyspark_job,\n",
    "                    # Other job configuration parameters if needed\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Wait for the operation to complete (blocking call)\n",
    "        while not operation.done:\n",
    "            pass  # You can add sleep here to avoid busy waiting\n",
    "\n",
    "        # Retrieve the result of the operation\n",
    "        result = operation.result()\n",
    "\n",
    "        print(f\"Job completed for the file {file_name}\")\n",
    "    else:\n",
    "        print(f\"No job configured for the file {file_name}\")\n",
    "\n",
    "    if file_name == '9.json':\n",
    "        # Configure the Dataproc client with the correct endpoint\n",
    "        client = dataproc.JobControllerClient(client_options={\n",
    "            \"api_endpoint\": \"us-central1-dataproc.googleapis.com:443\"\n",
    "        })\n",
    "\n",
    "        # Configure the PySpark job (adjust as needed)\n",
    "        pyspark_job = PySparkJob(main_python_file_uri='gs://bucket_prometheus_sc/ETL_functions/etl_google_review.py') \n",
    "\n",
    "        # Configure the entire job\n",
    "        job = Job(\n",
    "            pyspark_job=pyspark_job,\n",
    "            # Other job configuration parameters if needed\n",
    "        )\n",
    "\n",
    "        # Specify the cluster where the job will run\n",
    "        cluster_name = \"cluster-etl-spark\"\n",
    "\n",
    "        # Submit the job to Dataproc and get the operation\n",
    "        operation = client.submit_job(\n",
    "            request={\n",
    "                \"project_id\": \"prometheus-data-solutions\",\n",
    "                \"region\": \"us-central1\",\n",
    "                \"job\": {\n",
    "                    \"placement\": {\"cluster_name\": cluster_name},\n",
    "                    \"pyspark_job\": pyspark_job,\n",
    "                    # Other job configuration parameters if needed\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Wait for the operation to complete (blocking call)\n",
    "        while not operation.done:\n",
    "            pass  # You can add sleep here to avoid busy waiting\n",
    "\n",
    "        # Retrieve the result of the operation\n",
    "        result = operation.result()\n",
    "\n",
    "        print(f\"Job completed for the file {file_name}\")\n",
    "    else:\n",
    "        print(f\"No job configured for the file {file_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
